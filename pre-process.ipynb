{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pycountry\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7948b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/emdat-data.xlsx')\n",
    "\n",
    "def calculate_duration(row):\n",
    "    try:\n",
    "        start_year = row['Start Year']\n",
    "        start_month = row['Start Month'] if pd.notna(row['Start Month']) else 1\n",
    "        start_day = row['Start Day'] if pd.notna(row['Start Day']) else 1\n",
    "        \n",
    "        end_year = row['End Year']\n",
    "        end_month = row['End Month'] if pd.notna(row['End Month']) else 12\n",
    "        end_day = row['End Day'] if pd.notna(row['End Day']) else 31\n",
    "        \n",
    "        # Si alguno de los años no está disponible, retornar None\n",
    "        if pd.isna(start_year) or pd.isna(end_year):\n",
    "            return None\n",
    "        \n",
    "        # Crear objetos datetime\n",
    "        start_date = datetime(int(start_year), int(start_month), int(start_day))\n",
    "        end_date = datetime(int(end_year), int(end_month), int(end_day))\n",
    "        \n",
    "        duration = (end_date - start_date).days\n",
    "        \n",
    "        return duration if duration >= 0 else 0\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def create_date_column(row, prefix):\n",
    "    try:\n",
    "        year = row[f'{prefix} Year']\n",
    "        month = row[f'{prefix} Month'] if pd.notna(row[f'{prefix} Month']) else 1\n",
    "        day = row[f'{prefix} Day'] if pd.notna(row[f'{prefix} Day']) else 1\n",
    "        \n",
    "        if pd.isna(year):\n",
    "            return pd.NaT\n",
    "        \n",
    "        return pd.to_datetime(f\"{int(year)}-{int(month):02d}-{int(day):02d}\", errors='coerce')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def iso_to_country_name(iso_code):\n",
    "\n",
    "    if pd.isna(iso_code):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_3=iso_code)\n",
    "        return country.name if country else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "df['Start Date'] = df.apply(lambda row: create_date_column(row, 'Start'), axis=1)\n",
    "df['End Date'] = df.apply(lambda row: create_date_column(row, 'End'), axis=1)\n",
    "\n",
    "df['Duration'] = df.apply(calculate_duration, axis=1)\n",
    "\n",
    "\n",
    "df['Country Name'] = df['ISO'].apply(iso_to_country_name)\n",
    "\n",
    "\n",
    "print(f\"Country Name creado - Valores no nulos: {df['Country Name'].notna().sum()} de {len(df)}\")\n",
    "print(\"\\nComparación ISO vs Country Name:\")\n",
    "print(df[['ISO', 'Country', 'Country Name']].head(20))\n",
    "\n",
    "\n",
    "columns_to_drop = ['DisNo.', \n",
    "                'Historic', \n",
    "                'Classification Key', \n",
    "                'Disaster Group', \n",
    "                'External IDs', \n",
    "                'Admin Units', \n",
    "                'Latituede', \n",
    "                'Longitude', \n",
    "                'Latitude', \n",
    "                'Associated Types', \n",
    "                'GADM Admin Units', \n",
    "                'OFDA/BHA Response', \n",
    "                'Appeal', \n",
    "                'CPI', \n",
    "                'Declaration', \n",
    "                'River Basin', \n",
    "                \"Total Damage, Adjusted ('000 US$)\", \n",
    "                \"Insured Damage, Adjusted ('000 US$)\", \n",
    "                \"Reconstruction Costs, Adjusted ('000 US$)\", \n",
    "                \"Disaster Subtype\", \"Origin\", \"Location\", \n",
    "                \"Reconstruction Costs ('000 US$)\", \n",
    "                \"AID Contribution ('000 US$)\", \n",
    "                \"Insured Damage ('000 US$)\",\n",
    "                \"AID Contribution ('000 US$)\", \n",
    "                \"Reconstruction Costs ('000 US$)\",\n",
    "                \"Insured Damage ('000 US$)\", \n",
    "                \"Start Month\", \n",
    "                \"Start Day\", \n",
    "                \"End Year\", \n",
    "                \"End Month\", \n",
    "                \"End Day\",\n",
    "                \"Country\",\n",
    "                \"Duration\",\n",
    "                \"Entry Date\",\n",
    "                \"Last Update\",\n",
    "                \"Total Damage ('000 US$)\",\n",
    "                \"No. Homeless\",\n",
    "                \"No. Affected\",\n",
    "                \"No. Injured\",\n",
    "                \"Magnitude Scale\",\n",
    "                \"Magnitude\"\n",
    "            ]\n",
    "\n",
    "\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    df = df.drop(columns=existing_columns_to_drop)\n",
    "    print(f\"Columnas eliminadas: {existing_columns_to_drop}\")\n",
    "else:\n",
    "    print(\"No se encontraron columnas para eliminar\")\n",
    "\n",
    "mapping = {\n",
    "    \"Mass movement (dry)\": \"Mass Movement\",\n",
    "    \"Mass movement (wet)\": \"Mass Movement\",\n",
    "    \"Glacial lake outburst flood\": \"Glacial Flood\",\n",
    "    \"Animal incident\": \"Animal Incident\",\n",
    "    \"Extreme temperature\": \"Extreme Temperature\",\n",
    "    \"Volcanic activity\": \"Volcanic Activity\"\n",
    "}\n",
    "\n",
    "df[\"Disaster Type\"] = df[\"Disaster Type\"].replace(mapping)\n",
    "\n",
    "df.to_csv('data/emdat-data-processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a25d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_data = pd.read_csv('data/emdat-data-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_study_data(file_path):\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            data = pd.read_csv(file_path)\n",
    "        else:\n",
    "            data = pd.read_excel(file_path)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: There was a parsing error while reading the file.\")\n",
    "        return None\n",
    "    \n",
    "def analyze_missing_values(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    missing_stats = pd.DataFrame({\n",
    "        'Column': data.columns,\n",
    "        'Missing_Count': data.isnull().sum().values,\n",
    "        'Missing_Percentage': (data.isnull().sum().values / len(data) * 100).round(2),\n",
    "        'Data_Type': data.dtypes.values\n",
    "    })\n",
    "    \n",
    "    missing_stats = missing_stats.sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "\n",
    "def get_basic_statistics(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    return data.describe()\n",
    "\n",
    "\n",
    "def analyze_data_types(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    dtype_counts = data.dtypes.value_counts()\n",
    "    return dtype_counts\n",
    "\n",
    "\n",
    "def get_unique_values_summary(data, max_unique=50):\n",
    "    \n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    unique_summary = pd.DataFrame({\n",
    "        'Column': data.columns,\n",
    "        'Unique_Count': [data[col].nunique() for col in data.columns],\n",
    "        'Sample_Values': [str(data[col].dropna().unique()[:3].tolist()) if data[col].nunique() <= max_unique \n",
    "                         else f\"{data[col].nunique()} unique values\" \n",
    "                         for col in data.columns]\n",
    "    })\n",
    "    \n",
    "    return unique_summary\n",
    "\n",
    "\n",
    "def summarize_study_data(data):\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    summary = {\n",
    "        'num_rows': data.shape[0],\n",
    "        'num_columns': data.shape[1],\n",
    "        'column_names': data.columns.tolist(),\n",
    "        'total_missing': data.isnull().sum().sum(),\n",
    "        'memory_usage': data.memory_usage(deep=True).sum() / 1024**2  # En MB\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "    \n",
    "if study_data is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1. RESUMEN GENERAL\")\n",
    "    print(\"=\"*80)\n",
    "    summary = summarize_study_data(study_data)\n",
    "    print(f\"Número de filas: {summary['num_rows']:,}\")\n",
    "    print(f\"Número de columnas: {summary['num_columns']}\")\n",
    "    print(f\"Total de valores faltantes: {summary['total_missing']:,}\")\n",
    "    print(f\"Uso de memoria: {summary['memory_usage']:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. ANÁLISIS DE VALORES FALTANTES (NaN)\")\n",
    "    print(\"=\"*80)\n",
    "    missing_analysis = analyze_missing_values(study_data)\n",
    "    print(\"\\nColumnas con valores faltantes:\")\n",
    "    print(missing_analysis[missing_analysis['Missing_Count'] > 0].to_string(index=False))\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. DISTRIBUCIÓN DE TIPOS DE DATOS\")\n",
    "    print(\"=\"*80)\n",
    "    dtype_analysis = analyze_data_types(study_data)\n",
    "    print(dtype_analysis)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. ESTADÍSTICAS DESCRIPTIVAS (Columnas Numéricas)\")\n",
    "    print(\"=\"*80)\n",
    "    stats = get_basic_statistics(study_data)\n",
    "    print(stats)\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"5. RESUMEN DE VALORES ÚNICOS\")\n",
    "    print(\"=\"*80)\n",
    "    unique_summary = get_unique_values_summary(study_data)\n",
    "    print(unique_summary.to_string(index=False))\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"6. PRIMERAS 5 FILAS DEL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    print(study_data.head())\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"7. INFORMACIÓN DETALLADA DE COLUMNAS\")\n",
    "    print(\"=\"*80)\n",
    "    study_data.info()\n",
    "else:\n",
    "    print(\"No se pudo cargar el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study_data[\"Disaster Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "country_name_mapping = {\n",
    "    \"Congo, The Democratic Republic of the\": \"Democratic Republic of the Congo\",\n",
    "    \"Iran, Islamic Republic of\": \"Iran\",\n",
    "    \"Tanzania, United Republic of\": \"Tanzania\",\n",
    "    \"Korea, Democratic People's Republic of\": \"North Korea\",\n",
    "    \"Korea, Republic of\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    \"Venezuela, Bolivarian Republic of\": \"Venezuela\",\n",
    "    \"Moldova, Republic of\": \"Moldova\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Bolivia, Plurinational State of\": \"Bolivia\",\n",
    "    \"Macedonia, The Former Yugoslav Republic of\": \"North Macedonia\",\n",
    "    \"Czechia\": \"Czech Republic\",\n",
    "    \"Cabo Verde\": \"Cape Verde\",\n",
    "    \"Cote d'Ivoire\": \"Ivory Coast\",\n",
    "    \"Côte d'Ivoire\": \"Ivory Coast\",\n",
    "    \"Türkiye\": \"Turkey\",\n",
    "    \"Brunei Darussalam\": \"Brunei\",\n",
    "    \"Micronesia, Federated States of\": \"Micronesia\",\n",
    "    \"Saint Vincent and the Grenadines\": \"St. Vincent and the Grenadines\",\n",
    "    \"Saint Lucia\": \"St. Lucia\",\n",
    "    \"Saint Kitts and Nevis\": \"St. Kitts and Nevis\",\n",
    "    \"Sao Tome and Principe\": \"São Tomé and Príncipe\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"United States of America\": \"United States\"\n",
    "}\n",
    "\n",
    "study_data['Country Name'] = study_data['Country Name'].replace(country_name_mapping)\n",
    "\n",
    "study_data.to_csv('data/emdat-data-processed.csv', index=False)\n",
    "print(study_data['Country Name'].unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "north_america_subregions = ['Northern America']\n",
    "south_america_subregions = ['Latin America and the Caribbean']\n",
    "\n",
    "\n",
    "def reclassify_americas(row):\n",
    "    if row['Region'] == 'Americas':\n",
    "        if row['Subregion'] in north_america_subregions:\n",
    "            return 'North America'\n",
    "        elif row['Subregion'] in south_america_subregions:\n",
    "            return 'South America'\n",
    "    return row['Region']\n",
    "\n",
    "study_data['Region'] = study_data.apply(reclassify_americas, axis=1)\n",
    "\n",
    "print(study_data['Region'].unique())\n",
    "\n",
    "\n",
    "print(study_data['Region'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "print(study_data[study_data['Region'].isin(['North America', 'South America'])][['Region', 'Subregion']].drop_duplicates())\n",
    "study_data = study_data.drop(columns=['Subregion'])\n",
    "\n",
    "study_data.to_csv('data/emdat-data-processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ae0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disasters_by_country = study_data.groupby(['ISO', 'Country Name', 'Region', 'Disaster Type', 'Disaster Subgroup']).size().reset_index(name='Disaster_Count')\n",
    "\n",
    "disasters_by_country = disasters_by_country.sort_values('Disaster_Count', ascending=False)\n",
    "\n",
    "\n",
    "disasters_by_country.to_csv('disasters_by_country.csv', index=False)\n",
    "\n",
    "\n",
    "print(f\"\\n{disasters_by_country.head(20)}\")\n",
    "\n",
    "print(disasters_by_country.groupby('Region')['Disaster_Count'].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2091839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "study_data = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "\n",
    "geojson_files = [\n",
    "    'data/geometria/africa.geojson',\n",
    "    'data/geometria/asia.geojson',\n",
    "    'data/geometria/europe.geojson',\n",
    "    'data/geometria/north_america.geojson',\n",
    "    'data/geometria/oceania.geojson',\n",
    "    'data/geometria/south_america.geojson'\n",
    "]\n",
    "\n",
    "country_mapping = {}\n",
    "\n",
    "for geojson_file in geojson_files:\n",
    "    if os.path.exists(geojson_file):\n",
    "        with open(geojson_file, 'r', encoding='utf-8') as f:\n",
    "            geojson = json.load(f)\n",
    "            \n",
    "        for feature in geojson['features']:\n",
    "            props = feature['properties']\n",
    "            iso_a3 = props.get('iso_a3')\n",
    "            standard_name = props.get('name')\n",
    "            \n",
    "            if iso_a3 and standard_name:\n",
    "                country_mapping[iso_a3] = standard_name\n",
    "\n",
    "\n",
    "\n",
    "for iso_code, standard_name in country_mapping.items():\n",
    "    study_data.loc[study_data['ISO'] == iso_code, 'Country Name'] = standard_name\n",
    "\n",
    "\n",
    "study_data.to_csv('data/disasters_by_country.csv', index=False)\n",
    "\n",
    "unique_countries = study_data[['ISO', 'Country Name', 'Region']].drop_duplicates().sort_values('Country Name')\n",
    "print(unique_countries.head(30).to_string(index=False))\n",
    "\n",
    "\n",
    "unmapped = study_data[~study_data['ISO'].isin(country_mapping.keys())]['Country Name'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(unmapped[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_files = [\n",
    "    'data/geometria/africa.geojson',\n",
    "    'data/geometria/asia.geojson',\n",
    "    'data/geometria/europe.geojson',\n",
    "    'data/geometria/north_america.geojson',\n",
    "    'data/geometria/oceania.geojson',\n",
    "    'data/geometria/south_america.geojson'\n",
    "]\n",
    "\n",
    "country_mapping = {}\n",
    "\n",
    "for geojson_file in geojson_files:\n",
    "    if os.path.exists(geojson_file):\n",
    "        with open(geojson_file, 'r', encoding='utf-8') as f:\n",
    "            geojson = json.load(f)\n",
    "            \n",
    "        for feature in geojson['features']:\n",
    "            props = feature['properties']\n",
    "            iso_a3 = props.get('iso_a3')\n",
    "            standard_name = props.get('name')\n",
    "            \n",
    "            if iso_a3 and standard_name:\n",
    "                country_mapping[iso_a3] = standard_name\n",
    "\n",
    "manual_mapping = {\n",
    "    'BRA': 'Brazil',\n",
    "    'COL': 'Colombia',\n",
    "    'PER': 'Peru',\n",
    "    'BOL': 'Bolivia',\n",
    "    'ARG': 'Argentina',\n",
    "    'VEN': 'Venezuela',\n",
    "    'ECU': 'Ecuador',\n",
    "    'CHL': 'Chile',\n",
    "    'URY': 'Uruguay',\n",
    "    'PRY': 'Paraguay',\n",
    "    'HKG': 'Hong Kong',\n",
    "    'REU': 'Réunion',\n",
    "    'TON': 'Tonga',\n",
    "    'VCT': 'St. Vin. and Gren.',\n",
    "    'GUY': 'Guyana',\n",
    "    'BRB': 'Barbados',\n",
    "    'CYM': 'Cayman Is.',\n",
    "    'MUS': 'Mauritius',\n",
    "    'FSM': 'Micronesia',\n",
    "    'COM': 'Comoros',\n",
    "    'GLP': 'Guadeloupe',\n",
    "    'MTQ': 'Martinique',\n",
    "    'MYT': 'Mayotte',\n",
    "    'NCL': 'New Caledonia',\n",
    "    'PYF': 'Fr. Polynesia',\n",
    "    'SXM': 'Sint Maarten',\n",
    "    'MAF': 'St-Martin',\n",
    "    'BLM': 'St-Barthélemy',\n",
    "    'VGB': 'British Virgin Is.',\n",
    "    'VIR': 'U.S. Virgin Is.',\n",
    "    'ASM': 'American Samoa',\n",
    "    'GUM': 'Guam',\n",
    "    'MNP': 'N. Mariana Is.',\n",
    "    'PRI': 'Puerto Rico',\n",
    "    'BMU': 'Bermuda',\n",
    "    'MSR': 'Montserrat',\n",
    "    'TCA': 'Turks and Caicos Is.',\n",
    "    'AIA': 'Anguilla',\n",
    "    'GUF': 'French Guiana',\n",
    "    'SUR': 'Suriname',\n",
    "    'NIU': 'Niue',\n",
    "    'COK': 'Cook Is.',\n",
    "    'WLF': 'Wallis and Futuna',\n",
    "    'TKL': 'Tokelau',\n",
    "    'PLW': 'Palau',\n",
    "    'STP': 'São Tomé and Principe',\n",
    "    'CPV': 'Cape Verde',\n",
    "    'SHN': 'St. Helena',\n",
    "    'MAC': 'Macao',\n",
    "    'SGP': 'Singapore',\n",
    "    'LIE': 'Liechtenstein',\n",
    "    'QAT': 'Qatar',\n",
    "    'KWT': 'Kuwait',\n",
    "    'BHR': 'Bahrain',\n",
    "    'PSE': 'Palestine',\n",
    "    'TWN': 'Taiwan',\n",
    "    'PRK': 'North Korea',\n",
    "    'KOR': 'South Korea',\n",
    "    'LCA': 'St. Lucia',\n",
    "    'DMA': 'Dominica',\n",
    "    'WSM': 'Samoa',\n",
    "    'MHL': 'Marshall Is.',\n",
    "    'GRD': 'Grenada',\n",
    "    'ATG': 'Antigua and Barb.',\n",
    "    'SYC': 'Seychelles',\n",
    "    'TUV': 'Tuvalu',\n",
    "    'MDV': 'Maldives',\n",
    "    'KIR': 'Kiribati',\n",
    "    'KNA': 'St. Kitts and Nevis',\n",
    "    'MLT': 'Malta'\n",
    "}\n",
    "country_mapping.update(manual_mapping)\n",
    "\n",
    "df_emdat = pd.read_csv('data/emdat-data-processed.csv')\n",
    "\n",
    "for iso_code, standard_name in country_mapping.items():\n",
    "    df_emdat.loc[df_emdat['ISO'] == iso_code, 'Country Name'] = standard_name\n",
    "\n",
    "df_emdat.to_csv('data/emdat-data-processed.csv', index=False)\n",
    "\n",
    "df_disasters = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "for iso_code, standard_name in country_mapping.items():\n",
    "    df_disasters.loc[df_disasters['ISO'] == iso_code, 'Country Name'] = standard_name\n",
    "\n",
    "df_disasters.to_csv('data/disasters_by_country.csv', index=False)\n",
    "\n",
    "unmapped = df_disasters[~df_disasters['ISO'].isin(country_mapping.keys())][['ISO', 'Country Name']].drop_duplicates()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"{len(unmapped)}\")\n",
    "    print(unmapped.to_string(index=False))\n",
    "else:\n",
    "    print(\"OK\")\n",
    "\n",
    "\n",
    "print(df_disasters[['ISO', 'Country Name', 'Region']].drop_duplicates().sort_values('Country Name').head(30).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92faa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Country Name': 'Country',\n",
    "    'Disaster Subgroup': 'disaster',\n",
    "    'Disaster_Count': 'disaster_count'\n",
    "})\n",
    "\n",
    "df['color_intensity'] = df['disaster_count']\n",
    "\n",
    "regions = df['Region'].unique()\n",
    "\n",
    "for region in regions:\n",
    "    region_df = df[df['Region'] == region][['Country', 'disaster', 'disaster_count', 'color_intensity']]\n",
    "    \n",
    "    filename = f\"data/disasters_{region.lower().replace(' ', '_')}.csv\"\n",
    "    \n",
    "    region_df.to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd942ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "df_world = df_disasters.rename(columns={\n",
    "    'Country Name': 'Country',\n",
    "    'Disaster Subgroup': 'disaster',\n",
    "    'Disaster_Count': 'disaster_count'\n",
    "})\n",
    "\n",
    "df_world['color_intensity'] = df['disaster_count']\n",
    "\n",
    "df_world = df[['Country', 'disaster', 'disaster_count', 'color_intensity']]\n",
    "\n",
    "df_world.to_csv('data/disasters_world.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0726de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "study_data = study_data[['Disaster Subgroup', 'Disaster Type', 'Event Name', 'Total Deaths', 'Total Affected', 'Start Date', 'End Date', 'Country Name', 'Region']].copy()\n",
    "\n",
    "study_data = study_data.rename(columns={\n",
    "    'Disaster Subgroup': 'disaster',\n",
    "    'Disaster Type': 'disaster_type',\n",
    "    'Event Name': 'event_name',\n",
    "    'Total Deaths': 'deaths',\n",
    "    'Total Affected': 'affected',\n",
    "    'Start Date': 'start',\n",
    "    'End Date': 'end',\n",
    "    'Country Name': 'Country'\n",
    "})\n",
    "\n",
    "regions = study_data['Region'].unique()\n",
    "\n",
    "for region in regions:\n",
    "    region_df = study_data[study_data['Region'] == region].drop(columns=['Region'])\n",
    "    \n",
    "    filename = f\"data/{region.lower().replace(' ', '_')}_disaster_info.csv\"\n",
    "\n",
    "    region_df.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
