{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f38fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7948b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Name creado - Valores no nulos: 10601 de 10622\n",
      "\n",
      "Comparación ISO vs Country Name:\n",
      "    ISO                           Country  \\\n",
      "0   BRA                            Brazil   \n",
      "1   USA          United States of America   \n",
      "2   RWA                            Rwanda   \n",
      "3   USA          United States of America   \n",
      "4   NGA                           Nigeria   \n",
      "5   COD  Democratic Republic of the Congo   \n",
      "6   POL                            Poland   \n",
      "7   PAN                            Panama   \n",
      "8   IDN                         Indonesia   \n",
      "9   AFG                       Afghanistan   \n",
      "10  GTM                         Guatemala   \n",
      "11  CRI                        Costa Rica   \n",
      "12  MOZ                        Mozambique   \n",
      "13  ZAF                      South Africa   \n",
      "14  NPL                             Nepal   \n",
      "15  NIC                         Nicaragua   \n",
      "16  IDN                         Indonesia   \n",
      "17  YEM                             Yemen   \n",
      "18  GMB                            Gambia   \n",
      "19  NPL                             Nepal   \n",
      "\n",
      "                             Country Name  \n",
      "0                                  Brazil  \n",
      "1                           United States  \n",
      "2                                  Rwanda  \n",
      "3                           United States  \n",
      "4                                 Nigeria  \n",
      "5   Congo, The Democratic Republic of the  \n",
      "6                                  Poland  \n",
      "7                                  Panama  \n",
      "8                               Indonesia  \n",
      "9                             Afghanistan  \n",
      "10                              Guatemala  \n",
      "11                             Costa Rica  \n",
      "12                             Mozambique  \n",
      "13                           South Africa  \n",
      "14                                  Nepal  \n",
      "15                              Nicaragua  \n",
      "16                              Indonesia  \n",
      "17                                  Yemen  \n",
      "18                                 Gambia  \n",
      "19                                  Nepal  \n",
      "Columnas eliminadas: ['DisNo.', 'Historic', 'Classification Key', 'Disaster Group', 'External IDs', 'Admin Units', 'Longitude', 'Latitude', 'Associated Types', 'GADM Admin Units', 'OFDA/BHA Response', 'Appeal', 'CPI', 'Declaration', 'River Basin', \"Total Damage, Adjusted ('000 US$)\", \"Insured Damage, Adjusted ('000 US$)\", \"Reconstruction Costs, Adjusted ('000 US$)\", 'Disaster Subtype', 'Origin', 'Location', \"Reconstruction Costs ('000 US$)\", \"AID Contribution ('000 US$)\", \"Insured Damage ('000 US$)\", \"AID Contribution ('000 US$)\", \"Reconstruction Costs ('000 US$)\", \"Insured Damage ('000 US$)\", 'Start Month', 'Start Day', 'End Year', 'End Month', 'End Day', 'Country', 'Duration', 'Entry Date', 'Last Update', \"Total Damage ('000 US$)\", 'No. Homeless', 'No. Affected', 'No. Injured', 'Magnitude Scale', 'Magnitude']\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Excel\n",
    "df = pd.read_excel('data/emdat-data.xlsx')\n",
    "\n",
    "# Crear la columna 'duration' calculando los días entre las fechas de inicio y fin\n",
    "def calculate_duration(row):\n",
    "    try:\n",
    "        # Crear fechas de inicio y fin\n",
    "        start_year = row['Start Year']\n",
    "        start_month = row['Start Month'] if pd.notna(row['Start Month']) else 1\n",
    "        start_day = row['Start Day'] if pd.notna(row['Start Day']) else 1\n",
    "        \n",
    "        end_year = row['End Year']\n",
    "        end_month = row['End Month'] if pd.notna(row['End Month']) else 12\n",
    "        end_day = row['End Day'] if pd.notna(row['End Day']) else 31\n",
    "        \n",
    "        # Si alguno de los años no está disponible, retornar None\n",
    "        if pd.isna(start_year) or pd.isna(end_year):\n",
    "            return None\n",
    "        \n",
    "        # Crear objetos datetime\n",
    "        start_date = datetime(int(start_year), int(start_month), int(start_day))\n",
    "        end_date = datetime(int(end_year), int(end_month), int(end_day))\n",
    "        \n",
    "        # Calcular la duración en días\n",
    "        duration = (end_date - start_date).days\n",
    "        \n",
    "        return duration if duration >= 0 else 0\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Crear columnas Start Date y End Date\n",
    "def create_date_column(row, prefix):\n",
    "    \"\"\"\n",
    "    Crea una fecha a partir de Year, Month, Day con valores por defecto si faltan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        year = row[f'{prefix} Year']\n",
    "        month = row[f'{prefix} Month'] if pd.notna(row[f'{prefix} Month']) else 1\n",
    "        day = row[f'{prefix} Day'] if pd.notna(row[f'{prefix} Day']) else 1\n",
    "        \n",
    "        if pd.isna(year):\n",
    "            return pd.NaT\n",
    "        \n",
    "        return pd.to_datetime(f\"{int(year)}-{int(month):02d}-{int(day):02d}\", errors='coerce')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def iso_to_country_name(iso_code):\n",
    "    \"\"\"\n",
    "    Convierte un código ISO-3 a nombre de país\n",
    "    \"\"\"\n",
    "    if pd.isna(iso_code):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_3=iso_code)\n",
    "        return country.name if country else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Aplicar la función para crear las columnas de fecha\n",
    "df['Start Date'] = df.apply(lambda row: create_date_column(row, 'Start'), axis=1)\n",
    "df['End Date'] = df.apply(lambda row: create_date_column(row, 'End'), axis=1)\n",
    "\n",
    "# Guardar el dataset actualizado\n",
    "\n",
    "# Aplicar la función para crear la columna duration\n",
    "df['Duration'] = df.apply(calculate_duration, axis=1)\n",
    "\n",
    "# Aplicar la conversión\n",
    "df['Country Name'] = df['ISO'].apply(iso_to_country_name)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Country Name creado - Valores no nulos: {df['Country Name'].notna().sum()} de {len(df)}\")\n",
    "print(\"\\nComparación ISO vs Country Name:\")\n",
    "print(df[['ISO', 'Country', 'Country Name']].head(20))\n",
    "\n",
    "# Eliminar las columnas especificadas\n",
    "columns_to_drop = ['DisNo.', \n",
    "                'Historic', \n",
    "                'Classification Key', \n",
    "                'Disaster Group', \n",
    "                'External IDs', \n",
    "                'Admin Units', \n",
    "                'Latituede', \n",
    "                'Longitude', \n",
    "                'Latitude', \n",
    "                'Associated Types', \n",
    "                'GADM Admin Units', \n",
    "                'OFDA/BHA Response', \n",
    "                'Appeal', \n",
    "                'CPI', \n",
    "                'Declaration', \n",
    "                'River Basin', \n",
    "                \"Total Damage, Adjusted ('000 US$)\", \n",
    "                \"Insured Damage, Adjusted ('000 US$)\", \n",
    "                \"Reconstruction Costs, Adjusted ('000 US$)\", \n",
    "                \"Disaster Subtype\", \"Origin\", \"Location\", \n",
    "                \"Reconstruction Costs ('000 US$)\", \n",
    "                \"AID Contribution ('000 US$)\", \n",
    "                \"Insured Damage ('000 US$)\",\n",
    "                \"AID Contribution ('000 US$)\", \n",
    "                \"Reconstruction Costs ('000 US$)\",\n",
    "                \"Insured Damage ('000 US$)\", \n",
    "                \"Start Month\", \n",
    "                \"Start Day\", \n",
    "                \"End Year\", \n",
    "                \"End Month\", \n",
    "                \"End Day\",\n",
    "                \"Country\",\n",
    "                \"Duration\",\n",
    "                \"Entry Date\",\n",
    "                \"Last Update\",\n",
    "                \"Total Damage ('000 US$)\",\n",
    "                \"No. Homeless\",\n",
    "                \"No. Affected\",\n",
    "                \"No. Injured\",\n",
    "                \"Magnitude Scale\",\n",
    "                \"Magnitude\"\n",
    "            ]\n",
    "\n",
    "# Verificar qué columnas existen antes de eliminarlas\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    df = df.drop(columns=existing_columns_to_drop)\n",
    "    print(f\"Columnas eliminadas: {existing_columns_to_drop}\")\n",
    "else:\n",
    "    print(\"No se encontraron columnas para eliminar\")\n",
    "\n",
    "mapping = {\n",
    "    \"Mass movement (dry)\": \"Mass Movement\",\n",
    "    \"Mass movement (wet)\": \"Mass Movement\",\n",
    "    \"Glacial lake outburst flood\": \"Glacial Flood\",\n",
    "    \"Animal incident\": \"Animal Incident\",\n",
    "    \"Extreme temperature\": \"Extreme Temperature\",\n",
    "    \"Volcanic activity\": \"Volcanic Activity\"\n",
    "}\n",
    "\n",
    "df[\"Disaster Type\"] = df[\"Disaster Type\"].replace(mapping)\n",
    "# Crear columna Country Name a partir del código ISO\n",
    "\n",
    "# Guardar el dataset procesado\n",
    "df.to_csv('data/emdat-data-processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a25d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_data = pd.read_csv('data/emdat-data-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1469d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANÁLISIS DEL DATASET\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "1. RESUMEN GENERAL\n",
      "================================================================================\n",
      "Número de filas: 10,622\n",
      "Número de columnas: 12\n",
      "Total de valores faltantes: 13,008\n",
      "Uso de memoria: 6.06 MB\n",
      "\n",
      "================================================================================\n",
      "2. ANÁLISIS DE VALORES FALTANTES (NaN)\n",
      "================================================================================\n",
      "\n",
      "Columnas con valores faltantes:\n",
      "        Column  Missing_Count  Missing_Percentage Data_Type\n",
      "    Event Name           7998               75.30    object\n",
      "  Total Deaths           3006               28.30   float64\n",
      "Total Affected           1983               18.67   float64\n",
      "  Country Name             21                0.20    object\n",
      "\n",
      "================================================================================\n",
      "3. DISTRIBUCIÓN DE TIPOS DE DATOS\n",
      "================================================================================\n",
      "object     9\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "4. ESTADÍSTICAS DESCRIPTIVAS (Columnas Numéricas)\n",
      "================================================================================\n",
      "         Start Year   Total Deaths  Total Affected\n",
      "count  10622.000000    7616.000000    8.639000e+03\n",
      "mean    2012.217379     222.251838    5.615388e+05\n",
      "std        7.671002    3997.092313    6.205882e+06\n",
      "min     2000.000000       1.000000    1.000000e+00\n",
      "25%     2005.000000       4.000000    6.120000e+02\n",
      "50%     2012.000000      12.000000    5.392000e+03\n",
      "75%     2019.000000      35.000000    5.052050e+04\n",
      "max     2025.000000  222570.000000    3.300000e+08\n",
      "\n",
      "================================================================================\n",
      "5. RESUMEN DE VALORES ÚNICOS\n",
      "================================================================================\n",
      "           Column  Unique_Count                                                                 Sample_Values\n",
      "Disaster Subgroup             6                             ['Hydrological', 'Climatological', 'Geophysical']\n",
      "    Disaster Type            13                                              ['Flood', 'Wildfire', 'Drought']\n",
      "       Event Name          1080                                                            1080 unique values\n",
      "              ISO           220                                                             220 unique values\n",
      "        Subregion            17 ['Latin America and the Caribbean', 'Northern America', 'Sub-Saharan Africa']\n",
      "           Region             5                                              ['Americas', 'Africa', 'Europe']\n",
      "       Start Year            26                                                            [2018, 2002, 2022]\n",
      "     Total Deaths           572                                                             572 unique values\n",
      "   Total Affected          4369                                                            4369 unique values\n",
      "       Start Date          5397                                                            5397 unique values\n",
      "         End Date          5441                                                            5441 unique values\n",
      "     Country Name           217                                                             217 unique values\n",
      "\n",
      "================================================================================\n",
      "6. PRIMERAS 5 FILAS DEL DATASET\n",
      "================================================================================\n",
      "  Disaster Subgroup Disaster Type Event Name  ISO  \\\n",
      "0      Hydrological         Flood        NaN  BRA   \n",
      "1    Climatological      Wildfire        NaN  USA   \n",
      "2      Hydrological         Flood        NaN  RWA   \n",
      "3    Climatological       Drought        NaN  USA   \n",
      "4      Hydrological         Flood        NaN  NGA   \n",
      "\n",
      "                         Subregion    Region  Start Year  Total Deaths  \\\n",
      "0  Latin America and the Caribbean  Americas        2018           4.0   \n",
      "1                 Northern America  Americas        2002           NaN   \n",
      "2               Sub-Saharan Africa    Africa        2022           3.0   \n",
      "3                 Northern America  Americas        2024           NaN   \n",
      "4               Sub-Saharan Africa    Africa        2000           NaN   \n",
      "\n",
      "   Total Affected  Start Date    End Date   Country Name  \n",
      "0           250.0  2018-02-14  2018-02-16         Brazil  \n",
      "1          1572.0  2002-06-08  2002-06-08  United States  \n",
      "2             NaN  2022-11-17  2022-11-18         Rwanda  \n",
      "3             NaN  2024-01-01  2024-12-01  United States  \n",
      "4           500.0  2000-09-20  2000-09-21        Nigeria  \n",
      "\n",
      "================================================================================\n",
      "7. INFORMACIÓN DETALLADA DE COLUMNAS\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10622 entries, 0 to 10621\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Disaster Subgroup  10622 non-null  object \n",
      " 1   Disaster Type      10622 non-null  object \n",
      " 2   Event Name         2624 non-null   object \n",
      " 3   ISO                10622 non-null  object \n",
      " 4   Subregion          10622 non-null  object \n",
      " 5   Region             10622 non-null  object \n",
      " 6   Start Year         10622 non-null  int64  \n",
      " 7   Total Deaths       7616 non-null   float64\n",
      " 8   Total Affected     8639 non-null   float64\n",
      " 9   Start Date         10622 non-null  object \n",
      " 10  End Date           10622 non-null  object \n",
      " 11  Country Name       10601 non-null  object \n",
      "dtypes: float64(2), int64(1), object(9)\n",
      "memory usage: 995.9+ KB\n"
     ]
    }
   ],
   "source": [
    "def load_study_data(file_path):\n",
    "    \"\"\"\n",
    "    Load study dataset from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file containing the study data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the loaded study data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Detectar si es CSV o Excel por la extensión\n",
    "        if file_path.endswith('.csv'):\n",
    "            data = pd.read_csv(file_path)\n",
    "        else:\n",
    "            data = pd.read_excel(file_path)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: There was a parsing error while reading the file.\")\n",
    "        return None\n",
    "    \n",
    "def analyze_missing_values(data):\n",
    "    \"\"\"\n",
    "    Analyze missing values (NaN) in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with missing value statistics per column.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    missing_stats = pd.DataFrame({\n",
    "        'Column': data.columns,\n",
    "        'Missing_Count': data.isnull().sum().values,\n",
    "        'Missing_Percentage': (data.isnull().sum().values / len(data) * 100).round(2),\n",
    "        'Data_Type': data.dtypes.values\n",
    "    })\n",
    "    \n",
    "    # Ordenar por cantidad de valores faltantes\n",
    "    missing_stats = missing_stats.sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "\n",
    "def get_basic_statistics(data):\n",
    "    \"\"\"\n",
    "    Get basic statistics for numerical columns.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Statistics for numerical columns.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    return data.describe()\n",
    "\n",
    "\n",
    "def analyze_data_types(data):\n",
    "    \"\"\"\n",
    "    Analyze data types in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with counts of each data type.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    dtype_counts = data.dtypes.value_counts()\n",
    "    return dtype_counts\n",
    "\n",
    "\n",
    "def get_unique_values_summary(data, max_unique=50):\n",
    "    \"\"\"\n",
    "    Get summary of unique values for each column.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame to analyze.\n",
    "    max_unique (int): Maximum number of unique values to display details.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Summary of unique values per column.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    unique_summary = pd.DataFrame({\n",
    "        'Column': data.columns,\n",
    "        'Unique_Count': [data[col].nunique() for col in data.columns],\n",
    "        'Sample_Values': [str(data[col].dropna().unique()[:3].tolist()) if data[col].nunique() <= max_unique \n",
    "                         else f\"{data[col].nunique()} unique values\" \n",
    "                         for col in data.columns]\n",
    "    })\n",
    "    \n",
    "    return unique_summary\n",
    "\n",
    "\n",
    "def summarize_study_data(data):\n",
    "    \"\"\"\n",
    "    Summarize the study dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing the study data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A summary of the dataset including number of rows, columns, and column names.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    summary = {\n",
    "        'num_rows': data.shape[0],\n",
    "        'num_columns': data.shape[1],\n",
    "        'column_names': data.columns.tolist(),\n",
    "        'total_missing': data.isnull().sum().sum(),\n",
    "        'memory_usage': data.memory_usage(deep=True).sum() / 1024**2  # En MB\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "# Cargar el dataset procesado' \n",
    "print(\"=\"*80)\n",
    "print(\"ANÁLISIS DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "    \n",
    "if study_data is not None:\n",
    "    # 1. Resumen básico\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1. RESUMEN GENERAL\")\n",
    "    print(\"=\"*80)\n",
    "    summary = summarize_study_data(study_data)\n",
    "    print(f\"Número de filas: {summary['num_rows']:,}\")\n",
    "    print(f\"Número de columnas: {summary['num_columns']}\")\n",
    "    print(f\"Total de valores faltantes: {summary['total_missing']:,}\")\n",
    "    print(f\"Uso de memoria: {summary['memory_usage']:.2f} MB\")\n",
    "    \n",
    "# 2. Análisis de valores faltantes\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. ANÁLISIS DE VALORES FALTANTES (NaN)\")\n",
    "    print(\"=\"*80)\n",
    "    missing_analysis = analyze_missing_values(study_data)\n",
    "    print(\"\\nColumnas con valores faltantes:\")\n",
    "    print(missing_analysis[missing_analysis['Missing_Count'] > 0].to_string(index=False))\n",
    "        \n",
    "    # 3. Tipos de datos\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. DISTRIBUCIÓN DE TIPOS DE DATOS\")\n",
    "    print(\"=\"*80)\n",
    "    dtype_analysis = analyze_data_types(study_data)\n",
    "    print(dtype_analysis)\n",
    "\n",
    "    # 4. Estadísticas básicas\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. ESTADÍSTICAS DESCRIPTIVAS (Columnas Numéricas)\")\n",
    "    print(\"=\"*80)\n",
    "    stats = get_basic_statistics(study_data)\n",
    "    print(stats)\n",
    "        \n",
    "    # 5. Valores únicos\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"5. RESUMEN DE VALORES ÚNICOS\")\n",
    "    print(\"=\"*80)\n",
    "    unique_summary = get_unique_values_summary(study_data)\n",
    "    print(unique_summary.to_string(index=False))\n",
    "        \n",
    "    # 6. Primeras filas\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"6. PRIMERAS 5 FILAS DEL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    print(study_data.head())\n",
    "        \n",
    "    # 7. Información de columnas\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"7. INFORMACIÓN DETALLADA DE COLUMNAS\")\n",
    "    print(\"=\"*80)\n",
    "    study_data.info()\n",
    "else:\n",
    "    print(\"No se pudo cargar el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14ab751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flood' 'Wildfire' 'Drought' 'Mass Movement' 'Volcanic Activity'\n",
      " 'Earthquake' 'Storm' 'Epidemic' 'Extreme Temperature' 'Infestation'\n",
      " 'Glacial Flood' 'Animal Incident' 'Impact']\n"
     ]
    }
   ],
   "source": [
    "print(study_data[\"Disaster Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6dc76aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de países estandarizados correctamente!\n",
      "\n",
      "Total de filas procesadas: 10622\n",
      "\n",
      "Países únicos después de la estandarización: 217\n",
      "\n",
      "Primeros países únicos:\n",
      "['Brazil' 'United States' 'Rwanda' 'Nigeria'\n",
      " 'Democratic Republic of the Congo' 'Poland' 'Panama' 'Indonesia'\n",
      " 'Afghanistan' 'Guatemala' 'Costa Rica' 'Mozambique' 'South Africa'\n",
      " 'Nepal' 'Nicaragua' 'Yemen' 'Gambia' 'Mongolia' 'China' 'Colombia']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('data/emdat-data-processed.csv')\n",
    "\n",
    "# Diccionario de mapeo para estandarizar nombres de países\n",
    "country_name_mapping = {\n",
    "    \"Congo, The Democratic Republic of the\": \"Democratic Republic of the Congo\",\n",
    "    \"Iran, Islamic Republic of\": \"Iran\",\n",
    "    \"Tanzania, United Republic of\": \"Tanzania\",\n",
    "    \"Korea, Democratic People's Republic of\": \"North Korea\",\n",
    "    \"Korea, Republic of\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    \"Venezuela, Bolivarian Republic of\": \"Venezuela\",\n",
    "    \"Moldova, Republic of\": \"Moldova\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Bolivia, Plurinational State of\": \"Bolivia\",\n",
    "    \"Macedonia, The Former Yugoslav Republic of\": \"North Macedonia\",\n",
    "    \"Czechia\": \"Czech Republic\",\n",
    "    \"Cabo Verde\": \"Cape Verde\",\n",
    "    \"Cote d'Ivoire\": \"Ivory Coast\",\n",
    "    \"Côte d'Ivoire\": \"Ivory Coast\",\n",
    "    \"Türkiye\": \"Turkey\",\n",
    "    \"Brunei Darussalam\": \"Brunei\",\n",
    "    \"Micronesia, Federated States of\": \"Micronesia\",\n",
    "    \"Saint Vincent and the Grenadines\": \"St. Vincent and the Grenadines\",\n",
    "    \"Saint Lucia\": \"St. Lucia\",\n",
    "    \"Saint Kitts and Nevis\": \"St. Kitts and Nevis\",\n",
    "    \"Sao Tome and Principe\": \"São Tomé and Príncipe\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"United States of America\": \"United States\"\n",
    "}\n",
    "\n",
    "# Aplicar el mapeo de nombres\n",
    "df['Country Name'] = df['Country Name'].replace(country_name_mapping)\n",
    "\n",
    "# Guardar el archivo procesado\n",
    "df.to_csv('data/emdat-data-processed.csv', index=False)\n",
    "\n",
    "print(\"Nombres de países estandarizados correctamente!\")\n",
    "print(f\"\\nTotal de filas procesadas: {len(df)}\")\n",
    "print(f\"\\nPaíses únicos después de la estandarización: {df['Country Name'].nunique()}\")\n",
    "print(\"\\nPrimeros países únicos:\")\n",
    "print(df['Country Name'].unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b507f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regiones únicas después de la transformación:\n",
      "['South America' 'North America' 'Africa' 'Europe' 'Asia' 'Oceania']\n",
      "\n",
      "Conteo de desastres por región:\n",
      "Region\n",
      "Asia             4161\n",
      "Africa           2225\n",
      "South America    1738\n",
      "Europe           1390\n",
      "North America     722\n",
      "Oceania           386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Verificación de transformación:\n",
      "          Region                        Subregion\n",
      "0  South America  Latin America and the Caribbean\n",
      "1  North America                 Northern America\n",
      "\n",
      "Dataset actualizado guardado.\n"
     ]
    }
   ],
   "source": [
    "# Definir qué subregiones pertenecen a América del Norte y América del Sur\n",
    "north_america_subregions = ['Northern America']\n",
    "south_america_subregions = ['Latin America and the Caribbean']\n",
    "\n",
    "# Función para reclasificar Americas\n",
    "def reclassify_americas(row):\n",
    "    if row['Region'] == 'Americas':\n",
    "        if row['Subregion'] in north_america_subregions:\n",
    "            return 'North America'\n",
    "        elif row['Subregion'] in south_america_subregions:\n",
    "            return 'South America'\n",
    "    return row['Region']\n",
    "\n",
    "# Aplicar la reclasificación\n",
    "study_data['Region'] = study_data.apply(reclassify_americas, axis=1)\n",
    "\n",
    "# Verificar los cambios\n",
    "print(\"Regiones únicas después de la transformación:\")\n",
    "print(study_data['Region'].unique())\n",
    "\n",
    "print(\"\\nConteo de desastres por región:\")\n",
    "print(study_data['Region'].value_counts())\n",
    "\n",
    "# Verificar qué subregiones se transformaron\n",
    "print(\"\\nVerificación de transformación:\")\n",
    "print(study_data[study_data['Region'].isin(['North America', 'South America'])][['Region', 'Subregion']].drop_duplicates())\n",
    "study_data = study_data.drop(columns=['Subregion'])\n",
    "# Guardar el dataset actualizado\n",
    "study_data.to_csv('data/emdat-data-processed.csv', index=False)\n",
    "print(\"\\nDataset actualizado guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd5ae0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de desastres por país creado:\n",
      "Total de países: 1000\n",
      "\n",
      "     ISO   Country Name         Region Disaster Type Disaster Subgroup  \\\n",
      "942  USA  United States  North America         Storm    Meteorological   \n",
      "153  CHN          China           Asia         Flood      Hydrological   \n",
      "158  CHN          China           Asia         Storm    Meteorological   \n",
      "383  IDN      Indonesia           Asia         Flood      Hydrological   \n",
      "711  PHL    Philippines           Asia         Storm    Meteorological   \n",
      "392  IND          India           Asia         Flood      Hydrological   \n",
      "940  USA  United States  North America         Flood      Hydrological   \n",
      "112  BRA         Brazil  South America         Flood      Hydrological   \n",
      "150  CHN          China           Asia    Earthquake       Geophysical   \n",
      "708  PHL    Philippines           Asia         Flood      Hydrological   \n",
      "396  IND          India           Asia         Storm    Meteorological   \n",
      "967  VNM       Viet Nam           Asia         Storm    Meteorological   \n",
      "445  JPN          Japan           Asia         Storm    Meteorological   \n",
      "4    AFG    Afghanistan           Asia         Flood      Hydrological   \n",
      "682  PAK       Pakistan           Asia         Flood      Hydrological   \n",
      "965  VNM       Viet Nam           Asia         Flood      Hydrological   \n",
      "944  USA  United States  North America      Wildfire    Climatological   \n",
      "381  IDN      Indonesia           Asia    Earthquake       Geophysical   \n",
      "558  MEX         Mexico  South America         Storm    Meteorological   \n",
      "183  COL       Colombia  South America         Flood      Hydrological   \n",
      "\n",
      "     Disaster_Count  \n",
      "942             380  \n",
      "153             224  \n",
      "158             218  \n",
      "383             215  \n",
      "711             208  \n",
      "392             205  \n",
      "940             123  \n",
      "112             117  \n",
      "150             114  \n",
      "708             109  \n",
      "396             107  \n",
      "967             102  \n",
      "445              94  \n",
      "4                91  \n",
      "682              90  \n",
      "965              85  \n",
      "944              84  \n",
      "381              82  \n",
      "558              82  \n",
      "183              77  \n",
      "\n",
      "Conteo por región:\n",
      "Region\n",
      "Asia             4161\n",
      "Africa           2216\n",
      "South America    1738\n",
      "Europe           1378\n",
      "North America     722\n",
      "Oceania           386\n",
      "Name: Disaster_Count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset con conteo de desastres por país (usando código ISO)\n",
    "disasters_by_country = study_data.groupby(['ISO', 'Country Name', 'Region', 'Disaster Type', 'Disaster Subgroup']).size().reset_index(name='Disaster_Count')\n",
    "\n",
    "# Ordenar por cantidad de desastres (de mayor a menor)\n",
    "disasters_by_country = disasters_by_country.sort_values('Disaster_Count', ascending=False)\n",
    "\n",
    "# Guardar el dataset\n",
    "disasters_by_country.to_csv('disasters_by_country.csv', index=False)\n",
    "\n",
    "print(\"Dataset de desastres por país creado:\")\n",
    "print(f\"Total de países: {len(disasters_by_country)}\")\n",
    "print(f\"\\n{disasters_by_country.head(20)}\")\n",
    "\n",
    "# Verificar la distribución por región\n",
    "print(\"\\nConteo por región:\")\n",
    "print(disasters_by_country.groupby('Region')['Disaster_Count'].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2091839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mapeo creado con 160 países\n",
      "✓ Nombres de países estandarizados exitosamente\n",
      "\n",
      "Total de países únicos: 217\n",
      "\n",
      "✓ Primeros 30 países estandarizados:\n",
      "ISO                    Country Name        Region\n",
      "AFG                     Afghanistan          Asia\n",
      "ALB                         Albania        Europe\n",
      "DZA                         Algeria        Africa\n",
      "ASM                  American Samoa       Oceania\n",
      "AGO                          Angola        Africa\n",
      "AIA                        Anguilla South America\n",
      "ATG             Antigua and Barbuda South America\n",
      "ARG                       Argentina South America\n",
      "ARM                         Armenia          Asia\n",
      "AUS                       Australia       Oceania\n",
      "AUT                         Austria        Europe\n",
      "AZE                      Azerbaijan          Asia\n",
      "BHS                         Bahamas South America\n",
      "BGD                      Bangladesh          Asia\n",
      "BRB                        Barbados South America\n",
      "BLR                         Belarus        Europe\n",
      "BEL                         Belgium        Europe\n",
      "BLZ                          Belize South America\n",
      "BEN                           Benin        Africa\n",
      "BMU                         Bermuda North America\n",
      "BTN                          Bhutan          Asia\n",
      "BOL Bolivia, Plurinational State of South America\n",
      "BIH                Bosnia and Herz.        Europe\n",
      "BWA                        Botswana        Africa\n",
      "BRA                          Brazil South America\n",
      "BGR                        Bulgaria        Europe\n",
      "BFA                    Burkina Faso        Africa\n",
      "BDI                         Burundi        Africa\n",
      "CPV                      Cabo Verde        Africa\n",
      "KHM                        Cambodia          Asia\n",
      "\n",
      "⚠ Advertencia: 61 países sin mapear:\n",
      "['Brazil' 'Colombia' 'Peru' 'Bolivia, Plurinational State of' 'Argentina'\n",
      " 'Venezuela, Bolivarian Republic of' 'Ecuador' 'Chile' 'Uruguay'\n",
      " 'Paraguay']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Leer el CSV\n",
    "df = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "# Lista de archivos GeoJSON\n",
    "geojson_files = [\n",
    "    'data/geometria/africa.geojson',\n",
    "    'data/geometria/asia.geojson',\n",
    "    'data/geometria/europe.geojson',\n",
    "    'data/geometria/north_america.geojson',\n",
    "    'data/geometria/oceania.geojson',\n",
    "    'data/geometria/south_america.geojson'\n",
    "]\n",
    "\n",
    "# Crear un diccionario de mapeo global basado en códigos ISO\n",
    "country_mapping = {}\n",
    "\n",
    "# Leer todos los GeoJSON y construir el mapeo\n",
    "for geojson_file in geojson_files:\n",
    "    if os.path.exists(geojson_file):\n",
    "        with open(geojson_file, 'r', encoding='utf-8') as f:\n",
    "            geojson = json.load(f)\n",
    "            \n",
    "        for feature in geojson['features']:\n",
    "            props = feature['properties']\n",
    "            iso_a3 = props.get('iso_a3')\n",
    "            standard_name = props.get('name')\n",
    "            \n",
    "            if iso_a3 and standard_name:\n",
    "                country_mapping[iso_a3] = standard_name\n",
    "\n",
    "print(f\"✓ Mapeo creado con {len(country_mapping)} países\")\n",
    "\n",
    "# Aplicar el mapeo usando los códigos ISO\n",
    "for iso_code, standard_name in country_mapping.items():\n",
    "    df.loc[df['ISO'] == iso_code, 'Country Name'] = standard_name\n",
    "\n",
    "# Guardar el archivo actualizado\n",
    "df.to_csv('data/disasters_by_country.csv', index=False)\n",
    "\n",
    "print(\"✓ Nombres de países estandarizados exitosamente\")\n",
    "print(f\"\\nTotal de países únicos: {df['Country Name'].nunique()}\")\n",
    "print(\"\\n✓ Primeros 30 países estandarizados:\")\n",
    "unique_countries = df[['ISO', 'Country Name', 'Region']].drop_duplicates().sort_values('Country Name')\n",
    "print(unique_countries.head(30).to_string(index=False))\n",
    "\n",
    "# Verificar si hay países sin mapear\n",
    "unmapped = df[~df['ISO'].isin(country_mapping.keys())]['Country Name'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"\\n⚠ Advertencia: {len(unmapped)} países sin mapear:\")\n",
    "    print(unmapped[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66bd94b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mapeo desde GeoJSON: 160 países\n",
      "Mapeo total con manual: 222 países\n",
      "\n",
      "================================================================================\n",
      "Estandarizando emdat-data-processed.csv...\n",
      "================================================================================\n",
      "Países únicos antes: 217\n",
      "✓ Archivo actualizado\n",
      "Países únicos después: 217\n",
      "\n",
      "================================================================================\n",
      "Estandarizando disasters_by_country.csv...\n",
      "================================================================================\n",
      "Países únicos antes: 217\n",
      "Archivo actualizado\n",
      "Países únicos después: 217\n",
      "\n",
      "✓ ¡Todos los países han sido mapeados correctamente!\n",
      "\n",
      "================================================================================\n",
      "RESUMEN FINAL\n",
      "================================================================================\n",
      "Total países mapeados: 222\n",
      "Total países únicos en dataset: 217\n",
      "Países sin mapear: 0\n",
      "Primeros 30 países estandarizados:\n",
      "ISO       Country Name        Region\n",
      "AFG        Afghanistan          Asia\n",
      "ALB            Albania        Europe\n",
      "DZA            Algeria        Africa\n",
      "ASM     American Samoa       Oceania\n",
      "AGO             Angola        Africa\n",
      "AIA           Anguilla South America\n",
      "ATG  Antigua and Barb. South America\n",
      "ARG          Argentina South America\n",
      "ARM            Armenia          Asia\n",
      "AUS          Australia       Oceania\n",
      "AUT            Austria        Europe\n",
      "AZE         Azerbaijan          Asia\n",
      "BHS            Bahamas South America\n",
      "BGD         Bangladesh          Asia\n",
      "BRB           Barbados South America\n",
      "BLR            Belarus        Europe\n",
      "BEL            Belgium        Europe\n",
      "BLZ             Belize South America\n",
      "BEN              Benin        Africa\n",
      "BMU            Bermuda North America\n",
      "BTN             Bhutan          Asia\n",
      "BOL            Bolivia South America\n",
      "BIH   Bosnia and Herz.        Europe\n",
      "BWA           Botswana        Africa\n",
      "BRA             Brazil South America\n",
      "VGB British Virgin Is. South America\n",
      "BGR           Bulgaria        Europe\n",
      "BFA       Burkina Faso        Africa\n",
      "BDI            Burundi        Africa\n",
      "KHM           Cambodia          Asia\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANTE: Primero estandarizar el archivo emdat-data-processed.csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Lista de archivos GeoJSON\n",
    "geojson_files = [\n",
    "    'data/geometria/africa.geojson',\n",
    "    'data/geometria/asia.geojson',\n",
    "    'data/geometria/europe.geojson',\n",
    "    'data/geometria/north_america.geojson',\n",
    "    'data/geometria/oceania.geojson',\n",
    "    'data/geometria/south_america.geojson'\n",
    "]\n",
    "\n",
    "# Crear un diccionario de mapeo global basado en códigos ISO\n",
    "country_mapping = {}\n",
    "\n",
    "# Leer todos los GeoJSON y construir el mapeo\n",
    "for geojson_file in geojson_files:\n",
    "    if os.path.exists(geojson_file):\n",
    "        with open(geojson_file, 'r', encoding='utf-8') as f:\n",
    "            geojson = json.load(f)\n",
    "            \n",
    "        for feature in geojson['features']:\n",
    "            props = feature['properties']\n",
    "            iso_a3 = props.get('iso_a3')\n",
    "            standard_name = props.get('name')\n",
    "            \n",
    "            if iso_a3 and standard_name:\n",
    "                country_mapping[iso_a3] = standard_name\n",
    "\n",
    "print(f\"✓ Mapeo desde GeoJSON: {len(country_mapping)} países\")\n",
    "\n",
    "# Mapeo manual para países que faltan o tienen nombres diferentes\n",
    "manual_mapping = {\n",
    "    'BRA': 'Brazil',\n",
    "    'COL': 'Colombia',\n",
    "    'PER': 'Peru',\n",
    "    'BOL': 'Bolivia',\n",
    "    'ARG': 'Argentina',\n",
    "    'VEN': 'Venezuela',\n",
    "    'ECU': 'Ecuador',\n",
    "    'CHL': 'Chile',\n",
    "    'URY': 'Uruguay',\n",
    "    'PRY': 'Paraguay',\n",
    "    'HKG': 'Hong Kong',\n",
    "    'REU': 'Réunion',\n",
    "    'TON': 'Tonga',\n",
    "    'VCT': 'St. Vin. and Gren.',\n",
    "    'GUY': 'Guyana',\n",
    "    'BRB': 'Barbados',\n",
    "    'CYM': 'Cayman Is.',\n",
    "    'MUS': 'Mauritius',\n",
    "    'FSM': 'Micronesia',\n",
    "    'COM': 'Comoros',\n",
    "    'GLP': 'Guadeloupe',\n",
    "    'MTQ': 'Martinique',\n",
    "    'MYT': 'Mayotte',\n",
    "    'NCL': 'New Caledonia',\n",
    "    'PYF': 'Fr. Polynesia',\n",
    "    'SXM': 'Sint Maarten',\n",
    "    'MAF': 'St-Martin',\n",
    "    'BLM': 'St-Barthélemy',\n",
    "    'VGB': 'British Virgin Is.',\n",
    "    'VIR': 'U.S. Virgin Is.',\n",
    "    'ASM': 'American Samoa',\n",
    "    'GUM': 'Guam',\n",
    "    'MNP': 'N. Mariana Is.',\n",
    "    'PRI': 'Puerto Rico',\n",
    "    'BMU': 'Bermuda',\n",
    "    'MSR': 'Montserrat',\n",
    "    'TCA': 'Turks and Caicos Is.',\n",
    "    'AIA': 'Anguilla',\n",
    "    'GUF': 'French Guiana',\n",
    "    'SUR': 'Suriname',\n",
    "    'NIU': 'Niue',\n",
    "    'COK': 'Cook Is.',\n",
    "    'WLF': 'Wallis and Futuna',\n",
    "    'TKL': 'Tokelau',\n",
    "    'PLW': 'Palau',\n",
    "    'STP': 'São Tomé and Principe',\n",
    "    'CPV': 'Cape Verde',\n",
    "    'SHN': 'St. Helena',\n",
    "    'MAC': 'Macao',\n",
    "    'SGP': 'Singapore',\n",
    "    'LIE': 'Liechtenstein',\n",
    "    'QAT': 'Qatar',\n",
    "    'KWT': 'Kuwait',\n",
    "    'BHR': 'Bahrain',\n",
    "    'PSE': 'Palestine',\n",
    "    'TWN': 'Taiwan',\n",
    "    'PRK': 'North Korea',\n",
    "    'KOR': 'South Korea',\n",
    "    # Países pequeños faltantes\n",
    "    'LCA': 'St. Lucia',\n",
    "    'DMA': 'Dominica',\n",
    "    'WSM': 'Samoa',\n",
    "    'MHL': 'Marshall Is.',\n",
    "    'GRD': 'Grenada',\n",
    "    'ATG': 'Antigua and Barb.',\n",
    "    'SYC': 'Seychelles',\n",
    "    'TUV': 'Tuvalu',\n",
    "    'MDV': 'Maldives',\n",
    "    'KIR': 'Kiribati',\n",
    "    'KNA': 'St. Kitts and Nevis',\n",
    "    'MLT': 'Malta'\n",
    "}\n",
    "\n",
    "# Combinar ambos mapeos (manual sobrescribe GeoJSON si hay conflicto)\n",
    "country_mapping.update(manual_mapping)\n",
    "\n",
    "print(f\"Mapeo total con manual: {len(country_mapping)} países\")\n",
    "\n",
    "# 1. Estandarizar emdat-data-processed.csv\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Estandarizando emdat-data-processed.csv...\")\n",
    "print(\"=\"*80)\n",
    "df_emdat = pd.read_csv('data/emdat-data-processed.csv')\n",
    "print(f\"Países únicos antes: {df_emdat['Country Name'].nunique()}\")\n",
    "\n",
    "for iso_code, standard_name in country_mapping.items():\n",
    "    df_emdat.loc[df_emdat['ISO'] == iso_code, 'Country Name'] = standard_name\n",
    "\n",
    "df_emdat.to_csv('data/emdat-data-processed.csv', index=False)\n",
    "print(f\"✓ Archivo actualizado\")\n",
    "print(f\"Países únicos después: {df_emdat['Country Name'].nunique()}\")\n",
    "\n",
    "# 2. Estandarizar disasters_by_country.csv\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Estandarizando disasters_by_country.csv...\")\n",
    "print(\"=\"*80)\n",
    "df_disasters = pd.read_csv('data/disasters_by_country.csv')\n",
    "print(f\"Países únicos antes: {df_disasters['Country Name'].nunique()}\")\n",
    "\n",
    "for iso_code, standard_name in country_mapping.items():\n",
    "    df_disasters.loc[df_disasters['ISO'] == iso_code, 'Country Name'] = standard_name\n",
    "\n",
    "df_disasters.to_csv('data/disasters_by_country.csv', index=False)\n",
    "print(f\"Archivo actualizado\")\n",
    "print(f\"Países únicos después: {df_disasters['Country Name'].nunique()}\")\n",
    "\n",
    "# Verificar países sin mapear\n",
    "unmapped = df_disasters[~df_disasters['ISO'].isin(country_mapping.keys())][['ISO', 'Country Name']].drop_duplicates()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"Países aún sin mapear ({len(unmapped)}):\")\n",
    "    print(unmapped.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n✓ ¡Todos los países han sido mapeados correctamente!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total países mapeados: {len(country_mapping)}\")\n",
    "print(f\"Total países únicos en dataset: {df_disasters['Country Name'].nunique()}\")\n",
    "print(f\"Países sin mapear: {len(unmapped)}\")\n",
    "print(\"Primeros 30 países estandarizados:\")\n",
    "print(df_disasters[['ISO', 'Country Name', 'Region']].drop_duplicates().sort_values('Country Name').head(30).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92faa406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Creado: data/disasters_north_america.csv (15 registros)\n",
      "✓ Creado: data/disasters_asia.csv (267 registros)\n",
      "✓ Creado: data/disasters_south_america.csv (199 registros)\n",
      "✓ Creado: data/disasters_africa.csv (263 registros)\n",
      "✓ Creado: data/disasters_europe.csv (180 registros)\n",
      "✓ Creado: data/disasters_oceania.csv (76 registros)\n",
      "\n",
      "✓ Se han creado 6 datasets, uno por continente\n",
      "Continentes procesados: Africa, Asia, Europe, North America, Oceania, South America\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "# Renombrar las columnas\n",
    "df = df.rename(columns={\n",
    "    'Country Name': 'Country',\n",
    "    'Disaster Subgroup': 'disaster',\n",
    "    'Disaster_Count': 'disaster_count'\n",
    "})\n",
    "\n",
    "# Agregar la columna color_intensity igual a disaster_count\n",
    "df['color_intensity'] = df['disaster_count']\n",
    "\n",
    "# Obtener las regiones únicas\n",
    "regions = df['Region'].unique()\n",
    "\n",
    "# Crear un dataset para cada región/continente\n",
    "for region in regions:\n",
    "    # Filtrar datos por región y seleccionar solo las columnas necesarias\n",
    "    region_df = df[df['Region'] == region][['Country', 'disaster', 'disaster_count', 'color_intensity']]\n",
    "    \n",
    "    # Crear nombre de archivo basado en la región\n",
    "    filename = f\"data/disasters_{region.lower().replace(' ', '_')}.csv\"\n",
    "    \n",
    "    # Guardar el dataset\n",
    "    region_df.to_csv(filename, index=False)\n",
    "    print(f\"✓ Creado: {filename} ({len(region_df)} registros)\")\n",
    "\n",
    "print(f\"\\n✓ Se han creado {len(regions)} datasets, uno por continente\")\n",
    "print(f\"Continentes procesados: {', '.join(sorted(regions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd942ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Creado: disasters_world.csv (1000 registros)\n",
      "Países únicos: 217\n",
      "Tipos de desastres: ['Meteorological' 'Hydrological' 'Geophysical' 'Climatological'\n",
      " 'Biological' 'Extra-terrestrial']\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset mundial con todos los países\n",
    "df_world = pd.read_csv('data/disasters_by_country.csv')\n",
    "\n",
    "# Renombrar las columnas y agregar color_intensity\n",
    "df_world = df_disasters.rename(columns={\n",
    "    'Country Name': 'Country',\n",
    "    'Disaster Subgroup': 'disaster',\n",
    "    'Disaster_Count': 'disaster_count'\n",
    "})\n",
    "\n",
    "# Agregar la columna color_intensity igual a disaster_count\n",
    "df_world['color_intensity'] = df['disaster_count']\n",
    "\n",
    "# Seleccionar solo las columnas necesarias\n",
    "df_world = df[['Country', 'disaster', 'disaster_count', 'color_intensity']]\n",
    "\n",
    "# Guardar el dataset mundial\n",
    "df_world.to_csv('data/disasters_world.csv', index=False)\n",
    "print(f\"✓ Creado: disasters_world.csv ({len(df_world)} registros)\")\n",
    "print(f\"Países únicos: {df_world['Country'].nunique()}\")\n",
    "print(f\"Tipos de desastres: {df['disaster'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0726de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Creado: data/south_america_disaster_info.csv (1738 registros)\n",
      "✓ Creado: data/north_america_disaster_info.csv (722 registros)\n",
      "✓ Creado: data/africa_disaster_info.csv (2225 registros)\n",
      "✓ Creado: data/europe_disaster_info.csv (1390 registros)\n",
      "✓ Creado: data/asia_disaster_info.csv (4161 registros)\n",
      "✓ Creado: data/oceania_disaster_info.csv (386 registros)\n",
      "\n",
      "✓ Se han creado 6 archivos de información de desastres\n",
      "Continentes procesados: Africa, Asia, Europe, North America, Oceania, South America\n",
      "\n",
      "Columnas incluidas: disaster, event_name, deaths, affected, start, end, Country\n"
     ]
    }
   ],
   "source": [
    "# Crear archivos de información de desastres por continente\n",
    "df_info = pd.read_csv('data/emdat-data-processed.csv')\n",
    "\n",
    "# Seleccionar y renombrar las columnas necesarias\n",
    "df_info = df_info[['Disaster Subgroup', 'Disaster Type', 'Event Name', 'Total Deaths', 'Total Affected', 'Start Date', 'End Date', 'Country Name', 'Region']].copy()\n",
    "\n",
    "df_info = df_info.rename(columns={\n",
    "    'Disaster Subgroup': 'disaster',\n",
    "    'Disaster Type': 'disaster_type',\n",
    "    'Event Name': 'event_name',\n",
    "    'Total Deaths': 'deaths',\n",
    "    'Total Affected': 'affected',\n",
    "    'Start Date': 'start',\n",
    "    'End Date': 'end',\n",
    "    'Country Name': 'Country'\n",
    "})\n",
    "\n",
    "# Obtener las regiones únicas\n",
    "regions = df_info['Region'].unique()\n",
    "\n",
    "# Crear un archivo CSV para cada continente\n",
    "for region in regions:\n",
    "    # Filtrar datos por región y eliminar la columna Region\n",
    "    region_df = df_info[df_info['Region'] == region].drop(columns=['Region'])\n",
    "    \n",
    "    # Crear nombre de archivo basado en la región\n",
    "    filename = f\"data/{region.lower().replace(' ', '_')}_disaster_info.csv\"\n",
    "    \n",
    "    # Guardar el dataset\n",
    "    region_df.to_csv(filename, index=False)\n",
    "    print(f\"✓ Creado: {filename} ({len(region_df)} registros)\")\n",
    "\n",
    "print(f\"\\n✓ Se han creado {len(regions)} archivos de información de desastres\")\n",
    "print(f\"Continentes procesados: {', '.join(sorted(regions))}\")\n",
    "print(f\"\\nColumnas incluidas: disaster, event_name, deaths, affected, start, end, Country\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
